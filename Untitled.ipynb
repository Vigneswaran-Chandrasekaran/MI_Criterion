{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bin-wise pretraining model based on Information Theory and Hypergraph\n",
    "<br/>\n",
    "<sub>\n",
    "<pre>\n",
    "Algorithm\n",
    "1.For each hidden_layer (L_(i) : i = 1 to Last_h_layer):\n",
    "    1.1 W_(i -1) and b_(i) based on (Random Init section)\n",
    "    1.2 O_(i) = activation( W_(i-1).O_(i - 1) + b_(i)\n",
    "    1.3 for each neuron (n_(i)_(j): j 1 to Layer_size_(i))\n",
    "        1.3.1 MI_(i)_(j) = Estimate MI based on (proposed section: MI estimation)\n",
    "    1.4 bin_size_(i) = Bin_size_setting_for_layer(i) based on section()\n",
    "    1.5 iteration = 0\n",
    "    1.6 create bins_(i) based on M(i)\n",
    "    1.7 while iteration < max_iter (and) not k_helly_property( bins_(i))\n",
    "        1.7.1 for each bins_(i)_(k): k = 1 to bin_size_(i)\n",
    "            1.7.1.1 calculate decay_factor_(iteration) based on ()\n",
    "            1.7.1.2 calculate divergence measure_bin_(k) based on ()\n",
    "            1.7.1.3 Update parameters_(i)_(k) based on (param updation)\n",
    "        1.7.2 MI_(i)_(j) = Estimate MI based on (proposed section: MI estimation)\n",
    "        1.7.3 update bins_(i) based on MI_(i)\n",
    "        1.7.4 iteration = iteration + 1\n",
    "</pre>\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries \n",
    "from torch.utils import data\n",
    "from memory_profiler import profile\n",
    "from fast_histogram import histogram1d, histogram2d\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "import torch.nn.functional as fun\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>\n",
    "Dataset downloading and preparing batches<br/>\n",
    "Dataset description: http://yann.lecun.com/exdb/mnist/ <br/>\n",
    "Training set length: 48_000 samples, Validation set length: 12_000 samples and Testing set length: 10_000 samples<br/>\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile     # to see the memory profile of the function uncomment the  @profile decorator\n",
    "def dataset_load(tr_batch_size, val_batch_size, val_split):\n",
    "    #Download and prepare dataset chunks by DataLoader\n",
    "    tic = time.time()\n",
    "    print(\"Downloading dataset and preparing DataLoader\")\n",
    "    master_dataset = dataset.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True )\n",
    "    test_dataset = dataset.MNIST(root = './data', train = False, transform = transforms.ToTensor())\n",
    "    #Train and validation data is split with specified ratio\n",
    "    train_dataset, val_dataset = data.random_split(master_dataset, (int(len(master_dataset)*(1.0-val_split)), int(len(master_dataset)*val_split)))\n",
    "    #define dataloaders with defined batch size for training and validation\n",
    "    train_loader = DataLoader(dataset = train_dataset, batch_size = tr_batch_size, shuffle = True)\n",
    "    # validation data is shuffled as validation set is used in pretraining and so to avoid any particular class bias\n",
    "    val_loader = DataLoader(dataset = val_dataset, batch_size = val_batch_size, shuffle = True)\n",
    "    # shuffling test dataset is not required\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = tr_batch_size, shuffle = False)\n",
    "    toc = time.time()\n",
    "    print(\"Finished preparing. Total time elasped: \"+str(toc - tic)+\" seconds\")\n",
    "    return( train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mutual_info(X, neurons, bins = 5):\n",
    "    #Estimate Mutual Information between Input data X and Neuron's activations\n",
    "    neuronal_MI = np.zeros(neurons.shape[1])\n",
    "    index = 0\n",
    "    for neuron in neurons.T:\n",
    "        if index % 100 == 0:\n",
    "            print(index, end = \" \", flush = True)\n",
    "        #loop over each neuron \n",
    "        # neuron is the activation of particular neuron for each input data X\n",
    "        for dim in X.T:\n",
    "            # loop over each dimension to estimate MI\n",
    "            # we assume each dimension is independent to each other..\n",
    "            # Ref: https://stats.stackexchange.com/questions/413511/mutual-information-between-multi-dimensional-and-single-dimensional-variables      \n",
    "            if np.amax(dim) != np.amin(dim):\n",
    "                #check whether the dimension have only one value throughout \n",
    "                #normalize the values for faster computation\n",
    "                dim = (dim - np.amin(dim)) / (np.amax(dim) - np.amin(dim))\n",
    "                neuron = (neuron - np.amin(neuron)) / (np.amax(neuron) - np.amin(neuron))\n",
    "                #build histogram for joint X and Y\n",
    "                bins_xy = histogram2d(dim, neuron, bins, range = [[0,1],[0,1]])\n",
    "                #histogram for X and Y marginal\n",
    "                bins_x = histogram1d(dim, bins, range = [0,1])\n",
    "                bins_y = histogram1d(neuron, bins, range = [0,1])\n",
    "                # check any sum is zero, although previos condition checks.. a defensive program\n",
    "                if np.sum(bins_x) != 0 and np.sum(bins_y) != 0 and np.sum(bins_xy) != 0:\n",
    "                    #calculate marginal probabilities\n",
    "                    p_x = bins_x / np.sum(bins_x)\n",
    "                    p_y = bins_y / np.sum(bins_y)\n",
    "                    #calculate joint probability\n",
    "                    p_xy = bins_xy / np.sum(bins_xy)\n",
    "                    #estimate entropy \n",
    "                    H_x = -1 * np.sum( p_x * np.log(p_x))\n",
    "                    H_y = -1 * np.sum(p_y * np.log(p_y))\n",
    "                    H_xy = -1 * np.sum(p_xy * np.log(p_xy))\n",
    "                    #Mutual Information of the particular dimension and neuron out put \n",
    "                    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "                    sum = H_x + H_y - H_xy\n",
    "                    # check any is NaN.. This occurs sometimes because, in p_X * log(p_X), sometimes p_X becomes 0\n",
    "                    # making the whole term zero, but however in numpy this turns to become NaN...\n",
    "                    # TODO: Find a good way to handle this\n",
    "                    if not math.isnan(sum):\n",
    "                        # add the MI value to the corresponding neuron index\n",
    "                        neuronal_MI[index] += sum\n",
    "        index += 1    \n",
    "    # return array of mutual information corresponding to each neuron\n",
    "    return(neuronal_MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile\n",
    "def pre_train_model(model, val_loader):\n",
    "    # list of all model's hidden layers\n",
    "    layers = [model.input_layer, model.hlayer1, model.hlayer2, model.hlayer3, model.hlayer4]\n",
    "    # Hyperparameter k-value which determines number of bins for the particular layer\n",
    "    # This is calaculated based on Partial Information Decomposition \n",
    "    k_value = [4, 8, 5, 3, 2]\n",
    "    # k-helly stopping criterion's k-value to chec degree of overlap\n",
    "    k_helly = [5, 5, 5, 5, 5]\n",
    "    # loop over the layers \n",
    "    for l_indx in range(len(layers)):\n",
    "        # get the layer's parameters and detach it for updation \n",
    "        print(\"Working on layer: \"+str(l_indx))\n",
    "        w_matrix = layers[l_indx].weight.data.clone().detach().numpy()\n",
    "        b_matrix = layers[l_indx].bias.data.clone().detach().numpy()\n",
    "        #load the input data X\n",
    "        # use the validation dataset images as X\n",
    "        for _, (images, _) in enumerate(val_loader):\n",
    "            # reshape images \n",
    "            images = images.reshape(-1, 28*28).clone().detach().numpy()\n",
    "            # find the activaion by,\n",
    "            # act( X.W_T + b)\n",
    "            activation = np.dot(images,w_matrix.T) + b_matrix \n",
    "            activation = 1/(1 + np.exp(-activation))\n",
    "            # Estimate Mutual Information for each neuron\n",
    "            tic_est = time.time()\n",
    "            print(\"Estimation Information Theorotic quantities\")\n",
    "            neuronal_MI = estimate_mutual_info(images, activation, bins = 5)\n",
    "            toc_est = time.time()\n",
    "            print(\"Elasped time for estimation: \"+str(round(toc_est-tic_est,1))+\" seconds\")\n",
    "            print(\"----- Remeber: this elasped time is commenstruate with time required for pre-training the particular layer -----\")\n",
    "            # Get the index of sorted neurons based on MI value\n",
    "            index_sorted = np.argsort(neuronal_MI)[::-1]\n",
    "            #create clusters of given k value\n",
    "            clusters = np.array_split(index_sorted, k_value[l_indx])\n",
    "            # calculate the bin's avergage MI\n",
    "            bin_avg = []\n",
    "            for i in clusters:\n",
    "                bin_avg.append(np.sum(neuronal_MI[np.ix_(i)]) / neuronal_MI[np.ix_(i)].shape[0])\n",
    "            print(\"This is initial phase\")\n",
    "            print(bin_avg)\n",
    "            print(\"parameters:\")\n",
    "            print(w_matrix)\n",
    "            # start tuning parameters !!!\n",
    "            # hyperparameters defined \n",
    "            iteration = 0\n",
    "            decay_factor = 0.001    # decay factor for step-size\n",
    "            stopping_criteria = True\n",
    "            while stopping_criteria:\n",
    "                print(\"Iteration: \"+str(iteration))\n",
    "                for bin in range(len(clusters)):\n",
    "                    #TODO: parameter updation very important.... High priority    \n",
    "                    # w_matrix have n number of rows with each row representing particular neuron connection\n",
    "                    w_matrix[np.ix_(clusters[bin])] -=  (max(bin_avg) - bin_avg[bin]) * (1/(iteration+1))*decay_factor\n",
    "                    b_matrix[np.ix_(clusters[bin])] -=  (max(bin_avg) - bin_avg[bin]) * (1/(iteration+1))*decay_factor\n",
    "                #calculate activation for the updated weights \n",
    "                activation = np.dot(images,w_matrix.T) + b_matrix \n",
    "                activation = 1/(1 + np.exp(-activation))\n",
    "                #estimate MI for the tuned parameters\n",
    "                neuronal_MI = estimate_mutual_info(images, activation, bins = 5)\n",
    "                index_sorted = np.argsort(neuronal_MI)[::-1]\n",
    "                clusters = np.array_split(index_sorted, k_value[l_indx])\n",
    "                # calculate the bin's avergage MI\n",
    "                bin_avg = []\n",
    "                for i in clusters:\n",
    "                    bin_avg.append(np.sum(neuronal_MI[np.ix_(i)]) / neuronal_MI[np.ix_(i)].shape[0])\n",
    "                print(bin_avg)\n",
    "                iteration += 1\n",
    "                if iteration == 5:\n",
    "                    stopping_criteria = False\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class defining the structure of Deep Neural Network model and layers characterstics\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, nh1, nh2, nh3, nh4, nh5, output_dim):\n",
    "        \n",
    "        super(DeepNN, self).__init__()\n",
    "        \n",
    "        #hyperparameter setting \n",
    "        self.input_dim = input_dim\n",
    "        self.nh1, self.nh2, self.nh3, self.nh4, self.nh5, = nh1, nh2, nh3, nh4, nh5\n",
    "        self.output_dim = output_dim\n",
    "        #layer definition \n",
    "        self.input_layer = nn.Linear(self.input_dim, self.nh1)\n",
    "        self.hlayer1 = nn.Linear(self.nh1, self.nh2)\n",
    "        self.hlayer2 = nn.Linear(self.nh2, self.nh3)\n",
    "        self.hlayer3 = nn.Linear(self.nh3, self.nh4)\n",
    "        self.hlayer4 = nn.Linear(self.nh4, self.nh5)\n",
    "        self.output_layer = nn.Linear(self.nh5, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #propogation of each layer\n",
    "        self.out1 = fun.softmax(self.input_layer(x))\n",
    "        self.out2 = fun.softmax(self.hlayer1(self.out1))\n",
    "        self.out3 = fun.softmax(self.hlayer2(self.out2))\n",
    "        self.out4 = fun.softmax(self.hlayer3(self.out3))\n",
    "        self.out5 = fun.softmax(self.hlayer4(self.out4))\n",
    "        self.out6 = fun.softmax(self.output_layer(self.out5))\n",
    "        \n",
    "        return fun.softmax(self.out6, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':        \n",
    "    tr_batch_size = 4800\n",
    "    val_batch_size = 12000\n",
    "    val_split = 0.2\n",
    "    train_loader, val_loader, test_loader = dataset_load(tr_batch_size, val_batch_size, val_split)\n",
    "    model = DeepNN(784, 800, 120, 20, 20, 20, 10)\n",
    "    print(\"Pretraining phase..\")\n",
    "    pre_trained_model = pre_train_model(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
